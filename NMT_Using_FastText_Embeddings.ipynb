{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT Using FastText Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/db-gb/Neural_Machine_Translator/blob/master/NMT_Using_FastText_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG4KM2SHnhta",
        "colab_type": "code",
        "outputId": "955ca700-88b0-4668-e4c8-92bc27492c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikRUHz8dU30T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from gensim.models import FastText\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.layers import Input, CuDNNLSTM, Embedding, Dense\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbMQlwQFIGdg",
        "colab_type": "text"
      },
      "source": [
        "# Neural Machine Translation Using FastText Word Embeddings\n",
        "One of the most exciting features of neural networks is their ability to train computers to generate novel text. This holds a lot of promise in the field of natural language translation. Human language is very complex, and every language has its nuances and exceptions. This makes the task all the more difficult. \n",
        "\n",
        "In this article, I will walk through the steps of creating a Neural Machine Translator that translates from English to Spanish using pre-trained FastText word embeddings. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blv1d6VENHzA",
        "colab_type": "text"
      },
      "source": [
        "We will use the English-Spanish sentence pairs available on http://www.manythings.org/anki/. Click [here](http://www.manythings.org/anki/spa-eng.zip) to download. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTA8INoynp0z",
        "colab_type": "code",
        "outputId": "f50ac9a6-6947-4491-9b4a-f7b301d51826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# http://www.manythings.org/anki/spa-eng.zip\n",
        "data_path = \"/content/drive/My Drive/Spanish_Bible_Translation/spa.txt\"\n",
        "\n",
        "# Read the file\n",
        "with open(data_path, 'r', encoding='utf-8') as p:\n",
        "  lines = p.read().split('\\n')\n",
        "\n",
        "  print(len(lines))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "122937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV3qV9itH4xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There are over 120000 samples. To conserve memory, we will randomly sample 50,000 out of the first 80,000\n",
        "num_samples = 50000\n",
        "lines_to_consider = 80000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb_sPpoXxAo1",
        "colab_type": "text"
      },
      "source": [
        "## Text Preprocessing\n",
        "Now that we've read the lines from the file, we will perform some preprocessing steps. In this case, we will convert the sentences to all lower-case and remove and special characters. I've borrowed the following code from Dipanjan Sarkar's excellent [tutorial](https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/feature%20engineering%20text%20data/Feature%20Engineering%20Text%20Data%20-%20Traditional%20Strategies.ipynb) [1]. This function will perform the preprocessing steps we want. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bfQI-xenzlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wpt = nltk.WordPunctTokenizer()\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(\"[^\\w\\s]\", \"\", doc);\n",
        "\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(tokens)\n",
        "    return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDz_BVGVx05u",
        "colab_type": "text"
      },
      "source": [
        "Looking through the training data, we see that the sentences start out short and tend to get longer as we proceed through the file. To remedy this, we randomize the selection of sentences that we will train our model on. This will make sure our model is validated on a similar set of sentences to the training data. Otherwise, our model's validation set will have longer sentences than the training set. This will cause our model to perform more poorly against the validation data.\n",
        "\n",
        "Using the function we created above, we normalize each of the sentences. Also, we add start and end tokens to our Spanish sentences. I will explain the need for these tokens a little later on when I describe the encoder-decoder model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCcU3Vg2n7Qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "# Store each of the English and Spanish language statements in two arrays\n",
        "eng_sents = []\n",
        "span_sents = []\n",
        "\n",
        "# Select a random set of indices. \n",
        "random_idx = random.sample(range(0, lines_to_consider-1), num_samples)\n",
        "\n",
        "# Grab the randomized lines\n",
        "lines_randomized = []\n",
        "for idx in random_idx:\n",
        "  lines_randomized.append(lines[idx])\n",
        "\n",
        "# Split the lines into English and Spanish equivalents\n",
        "for line in lines_randomized:\n",
        "  eng, span, _ = line.split('\\t')\n",
        "\n",
        "  # Use normalize function to normalize sentences before appending to arrays\n",
        "  eng_sents.append(normalize_document(eng))\n",
        "  # Add start and stop tokens to target language phrases \n",
        "  #span_sents.append('START_ ' + normalize_document(span) + ' _END')\n",
        "  span_sents.append(normalize_document(span))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T94wvr5vTCjF",
        "colab_type": "text"
      },
      "source": [
        "# Data Exploration\n",
        "Now that we've imported our sample sentences and normalized them, we will explore the sentences and the words to have a better understanding of what we're dealing with. First we will explore the English sentences, then the Spanish.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy39jlIN5LP_",
        "colab_type": "text"
      },
      "source": [
        "## English Text\n",
        "It's a good idea to see how long the longest sentence is, and the average length of all the sentences. This will help us establish how long we want our input sequences to be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY353CV_BCyq",
        "colab_type": "code",
        "outputId": "75c03be7-273f-42d3-854b-82ed58c1a79f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# list of all sentence lengths\n",
        "eng_sent_lens = [len(sent.split()) for sent in eng_sents]\n",
        "\n",
        "print(\"Average English sentence length: \", np.mean(eng_sent_lens))\n",
        "print(\"Longest English sentence length: \", np.max(eng_sent_lens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average English sentence length:  4.91958\n",
            "Longest English sentence length:  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdxgKMO89-p0",
        "colab_type": "code",
        "outputId": "0c8411d4-2c58-436a-b928-61ab43f8efe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "fig, ax1 = plt.subplots(figsize=(7, 4))\n",
        "\n",
        "# fixed bin size\n",
        "bins = np.arange(0, max(eng_sent_lens)+1, 1) # fixed bin size\n",
        "\n",
        "color = 'tab:blue'\n",
        "ax1.set_xlabel('# of Words')\n",
        "ax1.set_ylabel('Sentence Count')\n",
        "ax1.set_xlim([min(eng_sent_lens)-1, max(eng_sent_lens)+1])\n",
        "ax1.hist(eng_sent_lens, bins=bins, alpha=0.5, color=color)\n",
        "plt.title(\"Distribution of English Sentence Lengths\")\n",
        "\n",
        "\n",
        "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAEYCAYAAACju6QJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyAQuSVIPmcAlSeohE7gkST30fwGjUJhybdFq+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 504x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rEHCxQZWEPf",
        "colab_type": "text"
      },
      "source": [
        "Plotting the distribution of sentence lengths shows a very nice bell curve with an average length of about 5 words. We notice that more than 99% of sentences have a length of 8 or less. For now, we will set the input sequence length to 8.\n",
        "This will help us when we build our input matrix for our neural network. Shortening the input sentence length will make our input matrix smaller and less-memory intensive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rptxnHAdX_Rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define input sequence length. This is how many words our input matrix will make space for later on. \n",
        "input_seq_len = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiPGFFV3CYEZ",
        "colab_type": "code",
        "outputId": "8ad648e6-1c4a-4dbc-b538-12fb03e5de39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "unique, counts = np.unique(eng_sent_lens, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 43,\n",
              " 2: 1824,\n",
              " 3: 6207,\n",
              " 4: 11391,\n",
              " 5: 13176,\n",
              " 6: 10645,\n",
              " 7: 5435,\n",
              " 8: 1205,\n",
              " 9: 72,\n",
              " 10: 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgXTOP3yekor",
        "colab_type": "text"
      },
      "source": [
        "Next, we create a dictionary of all English words that appear in the corpus, as well as the count of each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T82aTctwXGgI",
        "colab_type": "code",
        "outputId": "ff2b973a-0817-4f19-9c6a-e7c92928e1de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define a dictionary to store all English words\n",
        "eng_dict = dict()\n",
        "\n",
        "# Loop through each sentence in corpus\n",
        "for sent in eng_sents:\n",
        "  # Split each sentence into words and loop through each word\n",
        "  for wrd in sent.split(\" \"):\n",
        "    # If the word is already in dictionary, increment its count\n",
        "    if wrd in eng_dict:\n",
        "      eng_dict[wrd] += 1\n",
        "    # Otherwise, instantiate its count to 1\n",
        "    else:\n",
        "      eng_dict[wrd] = 1\n",
        "\n",
        "print(\"Number of unique English words: \", len(eng_dict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique English words:  7983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPGcHgluhR5u",
        "colab_type": "text"
      },
      "source": [
        "There appear to be about 8000 unique English words used throughout the entire corpus. If we create a matrix with a column for each unique word, we will end up with quite a large matrix. Not every word is created equal, and if we could somehow exclude less important words, we might save ourselves a lot of memory.\n",
        "\n",
        "Let's group the words in terms of frequency and see what we find."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN0-pGG5hFfR",
        "colab_type": "code",
        "outputId": "0f405fef-4454-4581-d58f-c109ce346951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Create a counter object that will count word frequencies\n",
        "cnt = Counter()\n",
        "\n",
        "# Create a list of the counts of each word\n",
        "for wrd_cnt in list(eng_dict.values()):\n",
        "  # Add each word's frequency to the counter\n",
        "  cnt[wrd_cnt] += 1\n",
        "\n",
        "# Print most common word frequencies\n",
        "cnt.most_common(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2899),\n",
              " (2, 1212),\n",
              " (3, 669),\n",
              " (4, 437),\n",
              " (5, 327),\n",
              " (6, 220),\n",
              " (7, 196),\n",
              " (8, 135),\n",
              " (9, 118),\n",
              " (10, 97)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RbSenL9jeWT",
        "colab_type": "text"
      },
      "source": [
        "The above shows us that 2913 words appear only once, 1157 appeared twice, and so on. \n",
        "\n",
        "We see that over a third of the words appear only once, and about half appear two times or fewer. If we could eliminate these words that appear in only one or two sentences out of the whole corpus, we can make our input matrix quite a bit smaller in terms of column count. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gty1-eaxsORt",
        "colab_type": "code",
        "outputId": "61312ecd-15af-48a0-f2de-a0388db912ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create a list of only those words that appear more than twice in entire corpus\n",
        "eng_wrds = [wrd for wrd in eng_dict if eng_dict[wrd] > 2]\n",
        "\n",
        "print(\"Our model will train on {} English words.\".format(len(eng_wrds)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our model will train on 3872 English words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8V1kxYy5W_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng_wrds.append('_UNK_')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDJphZJAFfGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_words = sorted(list(eng_wrds))\n",
        "num_encoder_tokens = len(eng_wrds)\n",
        "del eng_wrds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i20clmrXqJfo",
        "colab_type": "text"
      },
      "source": [
        "## Spanish Text\n",
        "We will now perform the same analysis on the Spanish translations of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS4JGN-BqWzd",
        "colab_type": "code",
        "outputId": "c2c168f5-d46c-4504-f131-437d8f18040b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# list of all sentence lengths\n",
        "span_sent_lens = [len(sent.split()) for sent in span_sents]\n",
        "\n",
        "print(\"Average Spanish sentence length: \", np.mean(span_sent_lens))\n",
        "print(\"Longest Spanish sentence length: \", np.max(span_sent_lens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Spanish sentence length:  4.71468\n",
            "Longest Spanish sentence length:  15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7budpG57qyP8",
        "colab_type": "text"
      },
      "source": [
        "We see that the average Spanish sentence is also about 5 words long, which is in line with the average English sentence length. However, the longest English sentence was only 10 words long, while the longest Spanish sentence is 15 words long. An interesting analysis would be to inspect the sentences whose lengths differ by more than x words and see what's behind the difference. For now, we will ignore this difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c_LRdJRr93W",
        "colab_type": "code",
        "outputId": "3a3b2514-0fb4-432e-fe2d-f5a8394b2cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "fig, ax1 = plt.subplots(figsize=(7, 4))\n",
        "\n",
        "# fixed bin size\n",
        "bins = np.arange(0, max(span_sent_lens)+1, 1) # fixed bin size\n",
        "\n",
        "color = 'tab:blue'\n",
        "ax1.set_xlabel('# of Words')\n",
        "ax1.set_ylabel('Sentence Count')\n",
        "ax1.set_xlim([min(span_sent_lens)-1, max(span_sent_lens)+1])\n",
        "ax1.hist(span_sent_lens, bins=bins, alpha=0.5, color=color)\n",
        "plt.title(\"Distribution of Spanish Sentence Lengths\")\n",
        "\n",
        "\n",
        "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAEYCAYAAACju6QJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 504x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOrSrlYBsNDY",
        "colab_type": "code",
        "outputId": "a1ead83c-30ac-46d9-dc57-1d05c5d0bea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "unique, counts = np.unique(span_sent_lens, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 449,\n",
              " 2: 2993,\n",
              " 3: 8034,\n",
              " 4: 11971,\n",
              " 5: 11722,\n",
              " 6: 8220,\n",
              " 7: 4361,\n",
              " 8: 1614,\n",
              " 9: 473,\n",
              " 10: 122,\n",
              " 11: 30,\n",
              " 12: 9,\n",
              " 14: 1,\n",
              " 15: 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxFdZjLxsVFY",
        "colab_type": "text"
      },
      "source": [
        "We see that there is a slight positive skew (tail on the right) on the distribution of Spanish sentence lengths. However, the skew is small and only a tiny fraction of sentences have more than 9 words. This means we will be able to use a smaller matrix to contain our Spanish sentences without the risk of slicing off too many words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCQ6GmTMtbnL",
        "colab_type": "code",
        "outputId": "8b30271e-ede8-40e2-f96f-d31713164600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define a dictionary to store all Spanish words\n",
        "span_dict = dict()\n",
        "\n",
        "# Loop through each sentence in corpus\n",
        "for sent in span_sents:\n",
        "  # Split each sentence into words and loop through each word\n",
        "  for wrd in sent.split(\" \"):\n",
        "    # If the word is already in dictionary, increment its count\n",
        "    if wrd in span_dict:\n",
        "      span_dict[wrd] += 1\n",
        "    # Otherwise, instantiate its count to 1\n",
        "    else:\n",
        "      span_dict[wrd] = 1\n",
        "\n",
        "print(\"Number of unique Spanish words: \", len(span_dict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique Spanish words:  14826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU8bRpl4t4LC",
        "colab_type": "text"
      },
      "source": [
        "Whereas the sentence lengths seemed to line up rather closely with the English sentences, it seems that the number of unique words in the Spanish text is much higher than that of the English. \n",
        "\n",
        "This most likely has to do with differences in the morphological typology of the two languages. Spanish is a much more fusional language consisting of a wide variety of verb conjugations and moods. Each conjugation of a verb is effectively a different word according to our approach. \n",
        "\n",
        "English, on the other hand, is much more analytic. There are fewer conjugations and each word contains a smaller number of morphemes [2].\n",
        "\n",
        "An example of this is in the conjugation for the verb 'to go', or 'ir' in Spanish. \n",
        "\n",
        "English: I go, you go, he/she goes, they go, we go - > 8 unique words\n",
        "\n",
        "Spanish: yo voy, tu vas, el/ella va, nosotros vamos, ellos/ellas van -> 12 unique words\n",
        "\n",
        "This difference in morphological types is one of many reasons why creating the perfect translator is so difficult. However, for our purposes, we will have to live with this difficulty and see how our translator performs regardless."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5yoMkxcyAzQ",
        "colab_type": "code",
        "outputId": "bba61b6a-fb5d-48ce-cf73-84afca39a420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Create a counter object that will count word frequencies\n",
        "cnt = Counter()\n",
        "\n",
        "# Create a list of the counts of each word\n",
        "for wrd_cnt in list(span_dict.values()):\n",
        "  # Add each word's frequency to the counter\n",
        "  cnt[wrd_cnt] += 1\n",
        "\n",
        "# Print most common word frequencies\n",
        "cnt.most_common(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 6791),\n",
              " (2, 2365),\n",
              " (3, 1231),\n",
              " (4, 715),\n",
              " (5, 505),\n",
              " (6, 373),\n",
              " (7, 295),\n",
              " (8, 235),\n",
              " (9, 170),\n",
              " (10, 162)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS7ZlWGcyJN_",
        "colab_type": "text"
      },
      "source": [
        "Again we see that about half the unique words in the Spanish corpus appear a total of 2 or fewer times. In the interest of keeping our memory footprint smaller, we will eliminate these from our words to consider. Since we will have to account for these rare words somehow, we'll create a fallback \"Unknown\" word that catches these. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mszZ0iOuylQb",
        "colab_type": "code",
        "outputId": "7cf98688-96f0-42bd-c987-9a3ec0eb7cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create a list of only those words that appear more than twice in entire corpus\n",
        "span_wrds = [wrd for wrd in span_dict if span_dict[wrd] > 2]\n",
        "\n",
        "print(\"Our model will train on {} Spanish words.\".format(len(span_wrds)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our model will train on 5670 Spanish words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIsI3B4ecP2N",
        "colab_type": "text"
      },
      "source": [
        "### Start and Stop Tokens\n",
        "Now we add start and stop tokens to the output language sentences. The start token will serve as an offset to help us train our model to predict the first word of the output sentence. The stop token will serve as a flag for our model to finish translating once it predicts a stop token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbZd2HihetbK",
        "colab_type": "code",
        "outputId": "65985d50-a4e8-4097-a292-2e7f7c82acb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Add start and stop tokens to target language phrases\n",
        "span_sents = ['START_ ' + sent + ' _END' for sent in span_sents]\n",
        "\n",
        "# Append Start and Stop tokens to dictionary as well as UNK to handle unknown words\n",
        "span_wrds.append('START_')\n",
        "span_wrds.append('_END')\n",
        "span_wrds.append('_UNK_')\n",
        "print(len(span_wrds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXxUNqy_FkM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Having an output sequence length of 9 would cover about 98% of our sentences. Add 1 to that to account for the STOP token. \n",
        "output_seq_len = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG1IRyDomc63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_words = sorted(span_wrds)\n",
        "num_decoder_tokens = len(span_wrds)\n",
        "del span_wrds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu9Rg96WqpmT",
        "colab_type": "text"
      },
      "source": [
        "## Big Picture\n",
        "Up to now, we have successfully analyzed the training text for our neural machine translator. We have determined the sentence lengths of the English and Spanish sentences. We've added the Start and Stop tokens to the Spanish text. We've created dictionaries of all words in our corpora, and we've filtered to remove words that appear only 1-2 times. \n",
        "\n",
        "Next, we will import word embeddings which will allow us to convert words in our dictionary to numerical vectors that our neural machine translator will be able to understand. \n",
        "\n",
        "Ultimately, our aim is to create a multidimensional matrix that captures all of our text in numerical form for both languages. Our matrix must contain values for every word of every sentence. We know how many sentences we're training on, and we've determined a sequence length for both English and Spanish that will hold almost all sentences in the text. Now we need a way to convert the individual words into numbers. For this, we will use FastText word embeddings. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLR7dJammt9k",
        "colab_type": "text"
      },
      "source": [
        "# Import FastText Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422bfcsUFpwV",
        "colab_type": "text"
      },
      "source": [
        "Word embeddings are an extremely useful tool in NLP. They work off the premise that you can learn a lot about the meaning of words based on the contexts they appear in. Word embeddings are an attempt to capture these relationships between words in a matrix of real numbers. Words that appear in similar contexts will result in vectors that are closer to one another. They allow for enormous dimensionality reduction compared to traditional approaches such as Bag-of-Words, with the added benefit in capturing similarities in meaning. You can learn more about word embeddings [here](https://en.wikipedia.org/wiki/Word_embedding) [4]. \n",
        "\n",
        "There are many approaches to creating word embeddings. For this exercise, we will use the pretrained embeddings provided by Facebook's Fasttext. While we could train our own embeddings using only the training data, using pretrained embeddings should provide an advantage because Facebook has access to millions of texts. For this reason, FastText embeddings bring in much more context knowledge for the individual words. Our dataset provides nowhere near enough training material.\n",
        "\n",
        "Another advantage of using FastText is that it provides word embeddings for many different languages. You can find them [here](https://fasttext.cc/docs/en/pretrained-vectors.html) [3]. \n",
        "\n",
        "We begin by importing the English FastText vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uZEQ4t_oUFt",
        "colab_type": "code",
        "outputId": "86529658-33b5-4d97-e15c-dbc7d9d55a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "ft_eng = KeyedVectors.load(\"/content/drive/My Drive/Spanish_Bible_Translation/fasttext_gensim_en.model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKUNgbI2LgDD",
        "colab_type": "text"
      },
      "source": [
        "The FastTextKeyedVectors objects contains 300-dimensional vectors for every word that FastText was trained on. We will convert the words in our training data into these vectors. Each sentence will then be represented by the FastText vector for each word in order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7VCz935JcXX",
        "colab_type": "text"
      },
      "source": [
        "### Handling Out-of-Vocabulary (OOV) Words\n",
        "In any large corpus, you are bound to encounter words that the popular pretrained word embeddings like FastText and Word2Vec do not account for. You need a robust way to handle these. \n",
        "\n",
        "One approach is to create random word embeddings for OOV words. However, this introduces the risk of the OOV word embeddings being complete outliers in the word embedding vector space. A simple remedy for this is to take the average of all the other word embeddings in the corpus. This will make sure your OOV word embeddings are not complete outliers that skew our model too heavily.\n",
        "\n",
        "First, let's see how many words in our corpus are unaccounted for in the FastText embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "844l9PCuoJel",
        "colab_type": "code",
        "outputId": "44475bc5-dff2-44e5-c4f5-806430e394b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_oov_words = []\n",
        "\n",
        "for i, wrd in enumerate(input_words):\n",
        "  if wrd not in ft_eng.vocab:\n",
        "    input_oov_words.append(wrd)\n",
        "\n",
        "print(\"Number of English OOV words: \", len(input_oov_words))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of English OOV words:  7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abGM6QVpmV4N",
        "colab_type": "text"
      },
      "source": [
        "We see that the OOV words make up a very small portion of the total vocabulary, which suggests that we are okay to try the method outlined above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhab7AQe4lky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dictionary that has every word from our corpus, as well as its corresponding word embedding.\n",
        "# This code inspired by sbongo on Kaggle [4].\n",
        "input_embeddings_index = dict()\n",
        "\n",
        "# Loop through all English words we have selected to train on above\n",
        "for i, word in enumerate(input_words):\n",
        "  try:\n",
        "    embedding_vector = ft_eng.get_vector(word)\n",
        "    # if the word is not found in the FastText embedding, a KeyError is generated\n",
        "  except KeyError:\n",
        "    # in case of error, set word's value to None\n",
        "    embedding_vector = None\n",
        "  if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "    input_embeddings_index[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2krh2CHQ0UG",
        "colab_type": "code",
        "outputId": "6218b0fb-72ed-4a71-d977-db542409afb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "# Taking a look at our word embeddings matrix shows a 300-column matrix where each row represents a word from our corpus, and its 300 dimension representation. \n",
        "embeds_df = pd.DataFrame.from_dict(input_embeddings_index)\n",
        "\n",
        "embeds_df.transpose().head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.024656</td>\n",
              "      <td>0.314972</td>\n",
              "      <td>-0.005146</td>\n",
              "      <td>-0.246556</td>\n",
              "      <td>0.086365</td>\n",
              "      <td>-0.045476</td>\n",
              "      <td>-0.295176</td>\n",
              "      <td>0.219644</td>\n",
              "      <td>-0.152214</td>\n",
              "      <td>0.018355</td>\n",
              "      <td>-0.192909</td>\n",
              "      <td>0.010552</td>\n",
              "      <td>0.196710</td>\n",
              "      <td>-0.013367</td>\n",
              "      <td>-0.159703</td>\n",
              "      <td>-0.378519</td>\n",
              "      <td>0.193004</td>\n",
              "      <td>-0.051495</td>\n",
              "      <td>-0.119618</td>\n",
              "      <td>0.011138</td>\n",
              "      <td>0.075937</td>\n",
              "      <td>0.117314</td>\n",
              "      <td>0.090979</td>\n",
              "      <td>-0.158478</td>\n",
              "      <td>0.403316</td>\n",
              "      <td>-0.183122</td>\n",
              "      <td>0.069790</td>\n",
              "      <td>0.168916</td>\n",
              "      <td>-0.062730</td>\n",
              "      <td>0.206916</td>\n",
              "      <td>0.015576</td>\n",
              "      <td>0.017910</td>\n",
              "      <td>0.076775</td>\n",
              "      <td>-0.107903</td>\n",
              "      <td>0.013860</td>\n",
              "      <td>0.260337</td>\n",
              "      <td>-0.021298</td>\n",
              "      <td>0.126669</td>\n",
              "      <td>0.130145</td>\n",
              "      <td>-0.175388</td>\n",
              "      <td>...</td>\n",
              "      <td>0.110815</td>\n",
              "      <td>0.128429</td>\n",
              "      <td>-0.474402</td>\n",
              "      <td>0.083203</td>\n",
              "      <td>-0.070008</td>\n",
              "      <td>-0.047603</td>\n",
              "      <td>0.083251</td>\n",
              "      <td>0.060340</td>\n",
              "      <td>0.136434</td>\n",
              "      <td>0.741646</td>\n",
              "      <td>0.143054</td>\n",
              "      <td>0.044781</td>\n",
              "      <td>0.475872</td>\n",
              "      <td>0.257370</td>\n",
              "      <td>-0.068337</td>\n",
              "      <td>0.096999</td>\n",
              "      <td>-0.103513</td>\n",
              "      <td>0.500242</td>\n",
              "      <td>0.394369</td>\n",
              "      <td>0.090317</td>\n",
              "      <td>0.261285</td>\n",
              "      <td>-0.049771</td>\n",
              "      <td>0.328350</td>\n",
              "      <td>-0.033421</td>\n",
              "      <td>0.107528</td>\n",
              "      <td>0.024151</td>\n",
              "      <td>-0.098062</td>\n",
              "      <td>-0.141807</td>\n",
              "      <td>0.148600</td>\n",
              "      <td>-0.029346</td>\n",
              "      <td>-0.509539</td>\n",
              "      <td>0.082166</td>\n",
              "      <td>-0.471768</td>\n",
              "      <td>-0.396551</td>\n",
              "      <td>0.097199</td>\n",
              "      <td>0.054422</td>\n",
              "      <td>0.146011</td>\n",
              "      <td>1.785282</td>\n",
              "      <td>-0.124270</td>\n",
              "      <td>-0.138877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.077940</td>\n",
              "      <td>-0.013375</td>\n",
              "      <td>-0.040519</td>\n",
              "      <td>0.025240</td>\n",
              "      <td>0.007050</td>\n",
              "      <td>-0.056055</td>\n",
              "      <td>-0.245428</td>\n",
              "      <td>0.004008</td>\n",
              "      <td>-0.202918</td>\n",
              "      <td>-0.016053</td>\n",
              "      <td>0.177788</td>\n",
              "      <td>0.130013</td>\n",
              "      <td>0.020246</td>\n",
              "      <td>-0.044872</td>\n",
              "      <td>0.142649</td>\n",
              "      <td>-0.046941</td>\n",
              "      <td>0.032662</td>\n",
              "      <td>-0.070071</td>\n",
              "      <td>0.025097</td>\n",
              "      <td>0.048525</td>\n",
              "      <td>-0.138796</td>\n",
              "      <td>-0.090938</td>\n",
              "      <td>0.067453</td>\n",
              "      <td>-0.309068</td>\n",
              "      <td>0.018225</td>\n",
              "      <td>-0.091944</td>\n",
              "      <td>0.005626</td>\n",
              "      <td>0.122892</td>\n",
              "      <td>0.019711</td>\n",
              "      <td>0.071462</td>\n",
              "      <td>0.177539</td>\n",
              "      <td>0.027793</td>\n",
              "      <td>0.013847</td>\n",
              "      <td>-0.043586</td>\n",
              "      <td>-0.018087</td>\n",
              "      <td>0.100908</td>\n",
              "      <td>-0.044851</td>\n",
              "      <td>-0.012540</td>\n",
              "      <td>0.072664</td>\n",
              "      <td>-0.069090</td>\n",
              "      <td>...</td>\n",
              "      <td>0.203686</td>\n",
              "      <td>0.086665</td>\n",
              "      <td>-0.056651</td>\n",
              "      <td>0.097758</td>\n",
              "      <td>-0.166555</td>\n",
              "      <td>0.029125</td>\n",
              "      <td>0.077378</td>\n",
              "      <td>0.008948</td>\n",
              "      <td>-0.052109</td>\n",
              "      <td>0.079798</td>\n",
              "      <td>0.195495</td>\n",
              "      <td>-0.001645</td>\n",
              "      <td>0.015319</td>\n",
              "      <td>0.010317</td>\n",
              "      <td>0.111552</td>\n",
              "      <td>-0.177327</td>\n",
              "      <td>0.013760</td>\n",
              "      <td>0.039803</td>\n",
              "      <td>0.059674</td>\n",
              "      <td>0.097895</td>\n",
              "      <td>0.104846</td>\n",
              "      <td>0.102773</td>\n",
              "      <td>0.103458</td>\n",
              "      <td>-0.073722</td>\n",
              "      <td>0.082919</td>\n",
              "      <td>0.047832</td>\n",
              "      <td>-0.037986</td>\n",
              "      <td>-0.116443</td>\n",
              "      <td>0.124333</td>\n",
              "      <td>-0.038435</td>\n",
              "      <td>-0.097613</td>\n",
              "      <td>0.061389</td>\n",
              "      <td>-0.036170</td>\n",
              "      <td>0.024187</td>\n",
              "      <td>0.015093</td>\n",
              "      <td>0.007598</td>\n",
              "      <td>0.191991</td>\n",
              "      <td>0.720688</td>\n",
              "      <td>-0.128143</td>\n",
              "      <td>0.040287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.044351</td>\n",
              "      <td>-0.048641</td>\n",
              "      <td>0.055769</td>\n",
              "      <td>0.026978</td>\n",
              "      <td>-0.042711</td>\n",
              "      <td>0.043635</td>\n",
              "      <td>-0.205185</td>\n",
              "      <td>0.064957</td>\n",
              "      <td>-0.046559</td>\n",
              "      <td>0.037053</td>\n",
              "      <td>-0.001812</td>\n",
              "      <td>0.090937</td>\n",
              "      <td>0.081294</td>\n",
              "      <td>0.018214</td>\n",
              "      <td>-0.007236</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.039685</td>\n",
              "      <td>-0.133132</td>\n",
              "      <td>0.009637</td>\n",
              "      <td>0.030598</td>\n",
              "      <td>-0.079288</td>\n",
              "      <td>-0.046162</td>\n",
              "      <td>-0.012947</td>\n",
              "      <td>-0.204195</td>\n",
              "      <td>-0.055754</td>\n",
              "      <td>-0.019038</td>\n",
              "      <td>-0.010965</td>\n",
              "      <td>-0.010909</td>\n",
              "      <td>0.065727</td>\n",
              "      <td>0.049580</td>\n",
              "      <td>0.236541</td>\n",
              "      <td>0.061714</td>\n",
              "      <td>-0.036478</td>\n",
              "      <td>-0.071829</td>\n",
              "      <td>-0.012632</td>\n",
              "      <td>0.071946</td>\n",
              "      <td>-0.114191</td>\n",
              "      <td>0.064537</td>\n",
              "      <td>0.004451</td>\n",
              "      <td>-0.022068</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013155</td>\n",
              "      <td>-0.001146</td>\n",
              "      <td>-0.067044</td>\n",
              "      <td>0.080927</td>\n",
              "      <td>-0.224239</td>\n",
              "      <td>0.056297</td>\n",
              "      <td>0.080608</td>\n",
              "      <td>0.017235</td>\n",
              "      <td>0.082128</td>\n",
              "      <td>0.065650</td>\n",
              "      <td>0.020189</td>\n",
              "      <td>0.063570</td>\n",
              "      <td>-0.012870</td>\n",
              "      <td>-0.118848</td>\n",
              "      <td>0.037302</td>\n",
              "      <td>-0.063459</td>\n",
              "      <td>0.042612</td>\n",
              "      <td>0.011375</td>\n",
              "      <td>-0.023481</td>\n",
              "      <td>0.032677</td>\n",
              "      <td>0.086424</td>\n",
              "      <td>0.014566</td>\n",
              "      <td>0.068131</td>\n",
              "      <td>0.007103</td>\n",
              "      <td>-0.079223</td>\n",
              "      <td>0.041792</td>\n",
              "      <td>0.029811</td>\n",
              "      <td>-0.079439</td>\n",
              "      <td>0.112950</td>\n",
              "      <td>-0.094655</td>\n",
              "      <td>-0.062548</td>\n",
              "      <td>0.038971</td>\n",
              "      <td>0.114171</td>\n",
              "      <td>0.034482</td>\n",
              "      <td>0.020712</td>\n",
              "      <td>0.031935</td>\n",
              "      <td>0.033440</td>\n",
              "      <td>0.421183</td>\n",
              "      <td>-0.158060</td>\n",
              "      <td>-0.050049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.071127</td>\n",
              "      <td>-0.052138</td>\n",
              "      <td>0.033053</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>-0.072915</td>\n",
              "      <td>0.034676</td>\n",
              "      <td>-0.189997</td>\n",
              "      <td>0.058901</td>\n",
              "      <td>-0.035795</td>\n",
              "      <td>0.119473</td>\n",
              "      <td>0.004229</td>\n",
              "      <td>0.027618</td>\n",
              "      <td>0.086226</td>\n",
              "      <td>-0.000558</td>\n",
              "      <td>0.022017</td>\n",
              "      <td>-0.004881</td>\n",
              "      <td>0.039690</td>\n",
              "      <td>-0.138336</td>\n",
              "      <td>0.100832</td>\n",
              "      <td>0.006392</td>\n",
              "      <td>0.068019</td>\n",
              "      <td>-0.041607</td>\n",
              "      <td>0.019578</td>\n",
              "      <td>-0.130116</td>\n",
              "      <td>0.003432</td>\n",
              "      <td>-0.072940</td>\n",
              "      <td>-0.096088</td>\n",
              "      <td>0.079649</td>\n",
              "      <td>0.094946</td>\n",
              "      <td>0.113494</td>\n",
              "      <td>0.166653</td>\n",
              "      <td>0.052580</td>\n",
              "      <td>-0.023510</td>\n",
              "      <td>-0.024061</td>\n",
              "      <td>0.032528</td>\n",
              "      <td>-0.020452</td>\n",
              "      <td>-0.086625</td>\n",
              "      <td>0.048702</td>\n",
              "      <td>0.076384</td>\n",
              "      <td>-0.042883</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025875</td>\n",
              "      <td>-0.010705</td>\n",
              "      <td>-0.021999</td>\n",
              "      <td>0.108157</td>\n",
              "      <td>-0.119358</td>\n",
              "      <td>-0.033750</td>\n",
              "      <td>0.034378</td>\n",
              "      <td>-0.010788</td>\n",
              "      <td>0.112584</td>\n",
              "      <td>0.097591</td>\n",
              "      <td>0.029823</td>\n",
              "      <td>0.001609</td>\n",
              "      <td>-0.020624</td>\n",
              "      <td>-0.081367</td>\n",
              "      <td>0.038557</td>\n",
              "      <td>-0.075083</td>\n",
              "      <td>0.013383</td>\n",
              "      <td>0.039087</td>\n",
              "      <td>-0.048410</td>\n",
              "      <td>0.035676</td>\n",
              "      <td>-0.054946</td>\n",
              "      <td>-0.068180</td>\n",
              "      <td>0.068415</td>\n",
              "      <td>0.007598</td>\n",
              "      <td>0.020639</td>\n",
              "      <td>0.077927</td>\n",
              "      <td>0.003684</td>\n",
              "      <td>-0.055025</td>\n",
              "      <td>0.132997</td>\n",
              "      <td>-0.013155</td>\n",
              "      <td>-0.041505</td>\n",
              "      <td>0.060940</td>\n",
              "      <td>0.106922</td>\n",
              "      <td>0.048870</td>\n",
              "      <td>0.069704</td>\n",
              "      <td>-0.036852</td>\n",
              "      <td>0.104741</td>\n",
              "      <td>0.291753</td>\n",
              "      <td>-0.134808</td>\n",
              "      <td>-0.079173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.128920</td>\n",
              "      <td>0.218175</td>\n",
              "      <td>0.096493</td>\n",
              "      <td>-0.284055</td>\n",
              "      <td>0.125471</td>\n",
              "      <td>-0.072978</td>\n",
              "      <td>-0.240527</td>\n",
              "      <td>0.201555</td>\n",
              "      <td>0.036282</td>\n",
              "      <td>0.008057</td>\n",
              "      <td>-0.189451</td>\n",
              "      <td>-0.137324</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.053857</td>\n",
              "      <td>-0.203388</td>\n",
              "      <td>-0.325665</td>\n",
              "      <td>0.002380</td>\n",
              "      <td>-0.057158</td>\n",
              "      <td>-0.107943</td>\n",
              "      <td>-0.042375</td>\n",
              "      <td>0.056537</td>\n",
              "      <td>0.157085</td>\n",
              "      <td>0.078650</td>\n",
              "      <td>-0.003580</td>\n",
              "      <td>0.533602</td>\n",
              "      <td>-0.136650</td>\n",
              "      <td>0.100137</td>\n",
              "      <td>0.127980</td>\n",
              "      <td>-0.066924</td>\n",
              "      <td>0.204381</td>\n",
              "      <td>-0.112439</td>\n",
              "      <td>0.030671</td>\n",
              "      <td>0.071371</td>\n",
              "      <td>-0.067411</td>\n",
              "      <td>-0.059652</td>\n",
              "      <td>0.264293</td>\n",
              "      <td>-0.003106</td>\n",
              "      <td>0.168000</td>\n",
              "      <td>0.119118</td>\n",
              "      <td>-0.122315</td>\n",
              "      <td>...</td>\n",
              "      <td>0.060924</td>\n",
              "      <td>0.125038</td>\n",
              "      <td>-0.580910</td>\n",
              "      <td>0.104233</td>\n",
              "      <td>0.022834</td>\n",
              "      <td>0.052287</td>\n",
              "      <td>0.067629</td>\n",
              "      <td>-0.024458</td>\n",
              "      <td>0.044185</td>\n",
              "      <td>0.823713</td>\n",
              "      <td>0.113294</td>\n",
              "      <td>-0.088024</td>\n",
              "      <td>0.561191</td>\n",
              "      <td>0.291761</td>\n",
              "      <td>-0.146107</td>\n",
              "      <td>0.155947</td>\n",
              "      <td>-0.084289</td>\n",
              "      <td>0.522186</td>\n",
              "      <td>0.379913</td>\n",
              "      <td>0.026491</td>\n",
              "      <td>-0.009587</td>\n",
              "      <td>-0.036025</td>\n",
              "      <td>0.376412</td>\n",
              "      <td>0.085284</td>\n",
              "      <td>0.110576</td>\n",
              "      <td>0.018410</td>\n",
              "      <td>-0.035880</td>\n",
              "      <td>-0.180616</td>\n",
              "      <td>0.190566</td>\n",
              "      <td>0.089783</td>\n",
              "      <td>-0.518218</td>\n",
              "      <td>-0.049997</td>\n",
              "      <td>-0.514816</td>\n",
              "      <td>-0.298621</td>\n",
              "      <td>0.109297</td>\n",
              "      <td>-0.042697</td>\n",
              "      <td>-0.069323</td>\n",
              "      <td>1.793249</td>\n",
              "      <td>-0.020844</td>\n",
              "      <td>-0.120290</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       297       298       299\n",
              "0  0.024656  0.314972 -0.005146  ...  1.785282 -0.124270 -0.138877\n",
              "1 -0.077940 -0.013375 -0.040519  ...  0.720688 -0.128143  0.040287\n",
              "2  0.044351 -0.048641  0.055769  ...  0.421183 -0.158060 -0.050049\n",
              "3  0.071127 -0.052138  0.033053  ...  0.291753 -0.134808 -0.079173\n",
              "4  0.128920  0.218175  0.096493  ...  1.793249 -0.020844 -0.120290\n",
              "\n",
              "[5 rows x 300 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wHr2SvN-vrD",
        "colab_type": "text"
      },
      "source": [
        "We take the mean value for each of the 300 dimensions. This will be our word embedding for all words that are not part of the FastText vocabulary, as well as all words in our corpus that we excluded because they occurred to rarely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8pyxsagR-5m",
        "colab_type": "code",
        "outputId": "1b371c1f-aef0-4450-edac-a1911c1be940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "embeds_df.mean(axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     -0.000736\n",
              "1      0.007872\n",
              "2      0.009288\n",
              "3      0.066464\n",
              "4     -0.021904\n",
              "         ...   \n",
              "295   -0.000982\n",
              "296   -0.004612\n",
              "297    0.073512\n",
              "298    0.008928\n",
              "299   -0.009777\n",
              "Length: 300, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdCz9OVp6m4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Capture the average of the word embeddings columns as the word embedding for rare or OOV words \n",
        "rare_embedding = embeds_df.mean(axis=1).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Mj_QivoKCz",
        "colab_type": "text"
      },
      "source": [
        "We now have everything we need to create our input word embedding matrix that consist of the embeddings for every word of every sentence of our training corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lb0G0_VoLUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a matrix with a row for every word in our English dictionary, and 300 columns to hold the word embedding for each word\n",
        "input_embedding_matrix = np.zeros((len(input_words), 300))\n",
        "\n",
        "# Loop through each input word\n",
        "for i, word in enumerate(input_words):\n",
        "  # For any words not in our dictionary (rare words)\n",
        "  if word == '_UNK_':\n",
        "    input_embedding_matrix[i] = rare_embedding\n",
        "  else:\n",
        "    # Try to get embedding vector for words in dictionary\n",
        "    try:\n",
        "      embedding_vector = ft_eng.get_vector(word)\n",
        "      # if no FastText embedding exists, set it to None\n",
        "    except KeyError:\n",
        "      embedding_vector = None\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "      input_embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "      # any remaining OOV words will be set to our rare_embedding\n",
        "      input_embedding_matrix[i] = rare_embedding #input_oov_embeddings[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en8JpFmuoNOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Memory clean up\n",
        "del ft_eng\n",
        "del input_embeddings_index\n",
        "del input_oov_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoIPrnF-oZAK",
        "colab_type": "text"
      },
      "source": [
        "###  Spanish FastText Word Embeddings\n",
        "We repeat the above procedure for the Spanish language FastText embeddings. At the end we will have the output embedding vectors for every word of every sentence of the training corpus. First we import the Spanish language FastText embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyyplrTZDyOX",
        "colab_type": "code",
        "outputId": "ed3c692b-d0ed-4481-c034-1a170d3d61eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Original fasttext embeddings from https://fasttext.cc/\n",
        "ft_span = KeyedVectors.load(\"/content/drive/My Drive/Spanish_Bible_Translation/fasttext_gensim_span.model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nxcec1fBF0A",
        "colab_type": "text"
      },
      "source": [
        "As before, we check to see how many of our Spanish words do not have FastText word embeddings. It appears only 4 words have no embedding, so these should not impact our translator too much."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J1zZTyQee2C",
        "colab_type": "code",
        "outputId": "adebe3e1-2837-493d-f2b8-423a01ba3501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output_oov_words = []\n",
        "\n",
        "for i, wrd in enumerate(output_words):\n",
        "  if wrd not in ft_span.vocab:\n",
        "    output_oov_words.append(wrd)\n",
        "\n",
        "print(len(output_oov_words))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EJHbooXB9Fgz",
        "colab": {}
      },
      "source": [
        "# Create a dictionary that has every word from our corpus, as well as its corresponding word embedding.\n",
        "# This code inspired by sbongo on Kaggle [4].\n",
        "output_embeddings_index = dict()\n",
        "\n",
        "# Loop through all Spanish words we have selected to train on above\n",
        "for i, word in enumerate(output_words):\n",
        "  try:\n",
        "    embedding_vector = ft_span.get_vector(word)\n",
        "    # if the word is not found in the FastText embedding, a KeyError is generated\n",
        "  except KeyError:\n",
        "    # in case of error, set word's value to None\n",
        "    embedding_vector = None\n",
        "  if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "    output_embeddings_index[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMc78ad0CIK_",
        "colab_type": "code",
        "outputId": "7d943c5c-02ed-455e-d535-a28da2caded4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "# Taking a look at our word embeddings matrix shows a 300-column matrix where each row represents a word from our corpus, and its 300 dimension representation. \n",
        "embeds_df = pd.DataFrame.from_dict(output_embeddings_index)\n",
        "\n",
        "embeds_df.transpose().head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.058693</td>\n",
              "      <td>-0.041589</td>\n",
              "      <td>-0.527735</td>\n",
              "      <td>0.006583</td>\n",
              "      <td>-0.104411</td>\n",
              "      <td>0.005482</td>\n",
              "      <td>0.004553</td>\n",
              "      <td>0.105835</td>\n",
              "      <td>-0.054745</td>\n",
              "      <td>-0.298510</td>\n",
              "      <td>-0.118578</td>\n",
              "      <td>-0.431450</td>\n",
              "      <td>-0.010504</td>\n",
              "      <td>0.112606</td>\n",
              "      <td>-0.056283</td>\n",
              "      <td>-0.075606</td>\n",
              "      <td>-0.006208</td>\n",
              "      <td>-0.073235</td>\n",
              "      <td>-0.167497</td>\n",
              "      <td>-0.012825</td>\n",
              "      <td>0.112638</td>\n",
              "      <td>-0.062582</td>\n",
              "      <td>-0.326707</td>\n",
              "      <td>0.159855</td>\n",
              "      <td>0.096485</td>\n",
              "      <td>-0.180181</td>\n",
              "      <td>0.219583</td>\n",
              "      <td>-0.148596</td>\n",
              "      <td>-0.084921</td>\n",
              "      <td>-0.039229</td>\n",
              "      <td>0.063824</td>\n",
              "      <td>-0.009856</td>\n",
              "      <td>0.104775</td>\n",
              "      <td>-0.052031</td>\n",
              "      <td>-0.131556</td>\n",
              "      <td>0.034438</td>\n",
              "      <td>-0.158310</td>\n",
              "      <td>0.046923</td>\n",
              "      <td>0.029212</td>\n",
              "      <td>0.078373</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020548</td>\n",
              "      <td>0.048872</td>\n",
              "      <td>-0.073923</td>\n",
              "      <td>-0.005662</td>\n",
              "      <td>0.030709</td>\n",
              "      <td>-0.041838</td>\n",
              "      <td>-0.028091</td>\n",
              "      <td>-0.194271</td>\n",
              "      <td>-0.030416</td>\n",
              "      <td>-0.184524</td>\n",
              "      <td>-0.032476</td>\n",
              "      <td>0.075927</td>\n",
              "      <td>0.024776</td>\n",
              "      <td>0.009928</td>\n",
              "      <td>0.121283</td>\n",
              "      <td>0.021360</td>\n",
              "      <td>0.004154</td>\n",
              "      <td>0.026488</td>\n",
              "      <td>-0.105815</td>\n",
              "      <td>-0.374584</td>\n",
              "      <td>-0.072662</td>\n",
              "      <td>-0.230708</td>\n",
              "      <td>0.078123</td>\n",
              "      <td>0.175316</td>\n",
              "      <td>-0.025363</td>\n",
              "      <td>0.194796</td>\n",
              "      <td>0.174660</td>\n",
              "      <td>0.024799</td>\n",
              "      <td>-0.193744</td>\n",
              "      <td>0.373640</td>\n",
              "      <td>-0.078084</td>\n",
              "      <td>-0.060603</td>\n",
              "      <td>0.249706</td>\n",
              "      <td>0.124453</td>\n",
              "      <td>0.024376</td>\n",
              "      <td>-0.153220</td>\n",
              "      <td>0.144082</td>\n",
              "      <td>-0.049402</td>\n",
              "      <td>-0.004989</td>\n",
              "      <td>0.033802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.132480</td>\n",
              "      <td>-0.054931</td>\n",
              "      <td>-0.032891</td>\n",
              "      <td>-0.053500</td>\n",
              "      <td>0.030309</td>\n",
              "      <td>0.124964</td>\n",
              "      <td>0.016290</td>\n",
              "      <td>-0.009644</td>\n",
              "      <td>-0.020838</td>\n",
              "      <td>-0.097519</td>\n",
              "      <td>0.143255</td>\n",
              "      <td>-0.100711</td>\n",
              "      <td>0.010575</td>\n",
              "      <td>0.281683</td>\n",
              "      <td>-0.116263</td>\n",
              "      <td>0.006835</td>\n",
              "      <td>0.132812</td>\n",
              "      <td>0.266508</td>\n",
              "      <td>-0.037246</td>\n",
              "      <td>-0.094431</td>\n",
              "      <td>0.088146</td>\n",
              "      <td>-0.112180</td>\n",
              "      <td>0.036869</td>\n",
              "      <td>0.070075</td>\n",
              "      <td>0.166911</td>\n",
              "      <td>-0.151394</td>\n",
              "      <td>0.197180</td>\n",
              "      <td>-0.133099</td>\n",
              "      <td>-0.240479</td>\n",
              "      <td>0.032154</td>\n",
              "      <td>0.116656</td>\n",
              "      <td>-0.140631</td>\n",
              "      <td>-0.048385</td>\n",
              "      <td>0.057829</td>\n",
              "      <td>0.033916</td>\n",
              "      <td>-0.092650</td>\n",
              "      <td>-0.057931</td>\n",
              "      <td>0.081105</td>\n",
              "      <td>0.086339</td>\n",
              "      <td>0.018632</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.050717</td>\n",
              "      <td>0.030561</td>\n",
              "      <td>0.038318</td>\n",
              "      <td>0.075170</td>\n",
              "      <td>0.037254</td>\n",
              "      <td>-0.030475</td>\n",
              "      <td>0.082890</td>\n",
              "      <td>-0.193923</td>\n",
              "      <td>-0.032253</td>\n",
              "      <td>-0.031062</td>\n",
              "      <td>0.092064</td>\n",
              "      <td>-0.018970</td>\n",
              "      <td>0.069517</td>\n",
              "      <td>-0.276418</td>\n",
              "      <td>0.012819</td>\n",
              "      <td>-0.078946</td>\n",
              "      <td>0.067280</td>\n",
              "      <td>-0.148780</td>\n",
              "      <td>-0.130320</td>\n",
              "      <td>-0.158245</td>\n",
              "      <td>-0.101267</td>\n",
              "      <td>0.055957</td>\n",
              "      <td>0.039009</td>\n",
              "      <td>-0.042046</td>\n",
              "      <td>-0.030622</td>\n",
              "      <td>0.066231</td>\n",
              "      <td>0.105556</td>\n",
              "      <td>-0.025273</td>\n",
              "      <td>-0.038508</td>\n",
              "      <td>0.105128</td>\n",
              "      <td>0.037075</td>\n",
              "      <td>-0.046781</td>\n",
              "      <td>0.001975</td>\n",
              "      <td>0.132278</td>\n",
              "      <td>-0.001982</td>\n",
              "      <td>0.021163</td>\n",
              "      <td>-0.070082</td>\n",
              "      <td>-0.002103</td>\n",
              "      <td>0.087079</td>\n",
              "      <td>-0.015044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.007859</td>\n",
              "      <td>-0.034372</td>\n",
              "      <td>-0.011872</td>\n",
              "      <td>-0.040757</td>\n",
              "      <td>-0.004726</td>\n",
              "      <td>0.039421</td>\n",
              "      <td>0.002977</td>\n",
              "      <td>-0.058133</td>\n",
              "      <td>0.002826</td>\n",
              "      <td>-0.078625</td>\n",
              "      <td>0.033083</td>\n",
              "      <td>0.006869</td>\n",
              "      <td>-0.052795</td>\n",
              "      <td>0.127894</td>\n",
              "      <td>-0.089140</td>\n",
              "      <td>-0.032147</td>\n",
              "      <td>0.086867</td>\n",
              "      <td>0.119333</td>\n",
              "      <td>-0.005107</td>\n",
              "      <td>0.030621</td>\n",
              "      <td>0.091394</td>\n",
              "      <td>-0.062417</td>\n",
              "      <td>0.011172</td>\n",
              "      <td>-0.014489</td>\n",
              "      <td>-0.103943</td>\n",
              "      <td>-0.063625</td>\n",
              "      <td>0.009322</td>\n",
              "      <td>0.005114</td>\n",
              "      <td>-0.043434</td>\n",
              "      <td>0.039843</td>\n",
              "      <td>0.052110</td>\n",
              "      <td>0.048994</td>\n",
              "      <td>-0.010895</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>-0.054528</td>\n",
              "      <td>-0.078716</td>\n",
              "      <td>-0.015579</td>\n",
              "      <td>-0.035256</td>\n",
              "      <td>0.046471</td>\n",
              "      <td>0.075714</td>\n",
              "      <td>...</td>\n",
              "      <td>0.078344</td>\n",
              "      <td>0.021315</td>\n",
              "      <td>0.012892</td>\n",
              "      <td>-0.074059</td>\n",
              "      <td>-0.019222</td>\n",
              "      <td>-0.060335</td>\n",
              "      <td>0.069843</td>\n",
              "      <td>-0.179099</td>\n",
              "      <td>-0.082948</td>\n",
              "      <td>0.045213</td>\n",
              "      <td>0.048552</td>\n",
              "      <td>-0.011010</td>\n",
              "      <td>-0.082636</td>\n",
              "      <td>-0.216628</td>\n",
              "      <td>0.057387</td>\n",
              "      <td>-0.047953</td>\n",
              "      <td>0.089599</td>\n",
              "      <td>-0.074243</td>\n",
              "      <td>-0.092671</td>\n",
              "      <td>-0.083364</td>\n",
              "      <td>0.016391</td>\n",
              "      <td>0.060402</td>\n",
              "      <td>-0.081850</td>\n",
              "      <td>-0.109799</td>\n",
              "      <td>-0.033588</td>\n",
              "      <td>0.079046</td>\n",
              "      <td>0.038469</td>\n",
              "      <td>0.028729</td>\n",
              "      <td>-0.038549</td>\n",
              "      <td>0.070046</td>\n",
              "      <td>0.006804</td>\n",
              "      <td>-0.056254</td>\n",
              "      <td>-0.032808</td>\n",
              "      <td>0.083062</td>\n",
              "      <td>-0.046185</td>\n",
              "      <td>-0.026001</td>\n",
              "      <td>-0.151745</td>\n",
              "      <td>-0.143186</td>\n",
              "      <td>0.111745</td>\n",
              "      <td>-0.147825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.004968</td>\n",
              "      <td>-0.053274</td>\n",
              "      <td>0.003256</td>\n",
              "      <td>0.009641</td>\n",
              "      <td>0.032247</td>\n",
              "      <td>0.074152</td>\n",
              "      <td>-0.068705</td>\n",
              "      <td>0.067072</td>\n",
              "      <td>-0.026289</td>\n",
              "      <td>-0.099364</td>\n",
              "      <td>0.034504</td>\n",
              "      <td>0.053108</td>\n",
              "      <td>-0.060703</td>\n",
              "      <td>0.009227</td>\n",
              "      <td>-0.077179</td>\n",
              "      <td>-0.034140</td>\n",
              "      <td>0.015959</td>\n",
              "      <td>0.051782</td>\n",
              "      <td>0.026612</td>\n",
              "      <td>0.011297</td>\n",
              "      <td>0.019561</td>\n",
              "      <td>-0.022305</td>\n",
              "      <td>0.037677</td>\n",
              "      <td>0.018198</td>\n",
              "      <td>-0.131391</td>\n",
              "      <td>0.008999</td>\n",
              "      <td>0.039208</td>\n",
              "      <td>0.041001</td>\n",
              "      <td>-0.014766</td>\n",
              "      <td>-0.015572</td>\n",
              "      <td>0.074324</td>\n",
              "      <td>0.025748</td>\n",
              "      <td>-0.035986</td>\n",
              "      <td>0.086882</td>\n",
              "      <td>0.011030</td>\n",
              "      <td>-0.058316</td>\n",
              "      <td>0.040517</td>\n",
              "      <td>-0.010919</td>\n",
              "      <td>0.057479</td>\n",
              "      <td>0.114912</td>\n",
              "      <td>...</td>\n",
              "      <td>0.067278</td>\n",
              "      <td>0.011869</td>\n",
              "      <td>0.004981</td>\n",
              "      <td>-0.123163</td>\n",
              "      <td>-0.044862</td>\n",
              "      <td>-0.057186</td>\n",
              "      <td>0.138993</td>\n",
              "      <td>-0.059784</td>\n",
              "      <td>-0.034646</td>\n",
              "      <td>0.007394</td>\n",
              "      <td>0.075698</td>\n",
              "      <td>-0.038346</td>\n",
              "      <td>-0.050469</td>\n",
              "      <td>-0.115070</td>\n",
              "      <td>0.056834</td>\n",
              "      <td>-0.079381</td>\n",
              "      <td>0.067570</td>\n",
              "      <td>-0.078431</td>\n",
              "      <td>-0.080352</td>\n",
              "      <td>-0.042884</td>\n",
              "      <td>-0.024534</td>\n",
              "      <td>-0.054472</td>\n",
              "      <td>0.017779</td>\n",
              "      <td>-0.033156</td>\n",
              "      <td>-0.036985</td>\n",
              "      <td>0.096469</td>\n",
              "      <td>0.094754</td>\n",
              "      <td>0.136999</td>\n",
              "      <td>-0.078067</td>\n",
              "      <td>0.066235</td>\n",
              "      <td>-0.033942</td>\n",
              "      <td>-0.038360</td>\n",
              "      <td>-0.000601</td>\n",
              "      <td>0.108546</td>\n",
              "      <td>-0.080877</td>\n",
              "      <td>-0.102020</td>\n",
              "      <td>-0.113880</td>\n",
              "      <td>-0.095048</td>\n",
              "      <td>0.044751</td>\n",
              "      <td>-0.048760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.045520</td>\n",
              "      <td>0.028169</td>\n",
              "      <td>-0.504016</td>\n",
              "      <td>-0.050459</td>\n",
              "      <td>-0.060454</td>\n",
              "      <td>-0.042268</td>\n",
              "      <td>0.200465</td>\n",
              "      <td>0.005276</td>\n",
              "      <td>-0.186099</td>\n",
              "      <td>-0.294319</td>\n",
              "      <td>-0.127091</td>\n",
              "      <td>-0.428724</td>\n",
              "      <td>-0.027019</td>\n",
              "      <td>-0.014172</td>\n",
              "      <td>-0.053544</td>\n",
              "      <td>-0.063168</td>\n",
              "      <td>0.030309</td>\n",
              "      <td>-0.182305</td>\n",
              "      <td>-0.049363</td>\n",
              "      <td>-0.039748</td>\n",
              "      <td>0.284700</td>\n",
              "      <td>0.051976</td>\n",
              "      <td>-0.382282</td>\n",
              "      <td>0.168214</td>\n",
              "      <td>0.027301</td>\n",
              "      <td>-0.006547</td>\n",
              "      <td>0.145301</td>\n",
              "      <td>-0.045073</td>\n",
              "      <td>-0.024910</td>\n",
              "      <td>-0.087953</td>\n",
              "      <td>0.041879</td>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.055819</td>\n",
              "      <td>-0.017193</td>\n",
              "      <td>-0.020725</td>\n",
              "      <td>0.050376</td>\n",
              "      <td>-0.116668</td>\n",
              "      <td>-0.023626</td>\n",
              "      <td>0.031963</td>\n",
              "      <td>0.151822</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073474</td>\n",
              "      <td>0.036453</td>\n",
              "      <td>-0.037153</td>\n",
              "      <td>0.124963</td>\n",
              "      <td>-0.032837</td>\n",
              "      <td>0.063165</td>\n",
              "      <td>-0.150176</td>\n",
              "      <td>0.061467</td>\n",
              "      <td>-0.044118</td>\n",
              "      <td>-0.102079</td>\n",
              "      <td>0.019599</td>\n",
              "      <td>-0.073573</td>\n",
              "      <td>0.064649</td>\n",
              "      <td>0.095015</td>\n",
              "      <td>0.157862</td>\n",
              "      <td>-0.029224</td>\n",
              "      <td>0.008866</td>\n",
              "      <td>-0.022451</td>\n",
              "      <td>-0.041407</td>\n",
              "      <td>-0.348846</td>\n",
              "      <td>-0.002347</td>\n",
              "      <td>-0.155283</td>\n",
              "      <td>-0.152060</td>\n",
              "      <td>0.263832</td>\n",
              "      <td>-0.121119</td>\n",
              "      <td>0.009481</td>\n",
              "      <td>0.171542</td>\n",
              "      <td>-0.035907</td>\n",
              "      <td>-0.194524</td>\n",
              "      <td>0.304962</td>\n",
              "      <td>-0.121852</td>\n",
              "      <td>-0.045295</td>\n",
              "      <td>0.259177</td>\n",
              "      <td>0.042525</td>\n",
              "      <td>0.195257</td>\n",
              "      <td>-0.036841</td>\n",
              "      <td>-0.057779</td>\n",
              "      <td>-0.008067</td>\n",
              "      <td>0.042133</td>\n",
              "      <td>0.074184</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       297       298       299\n",
              "0 -0.058693 -0.041589 -0.527735  ... -0.049402 -0.004989  0.033802\n",
              "1 -0.132480 -0.054931 -0.032891  ... -0.002103  0.087079 -0.015044\n",
              "2 -0.007859 -0.034372 -0.011872  ... -0.143186  0.111745 -0.147825\n",
              "3 -0.004968 -0.053274  0.003256  ... -0.095048  0.044751 -0.048760\n",
              "4  0.045520  0.028169 -0.504016  ... -0.008067  0.042133  0.074184\n",
              "\n",
              "[5 rows x 300 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ2j7SfECPJf",
        "colab_type": "code",
        "outputId": "8e15d941-1b94-4bab-f6b6-aaa18ca033e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "embeds_df.mean(axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0.000695\n",
              "1     -0.020730\n",
              "2      0.013803\n",
              "3     -0.002537\n",
              "4      0.005876\n",
              "         ...   \n",
              "295   -0.001876\n",
              "296    0.000396\n",
              "297   -0.008621\n",
              "298    0.011527\n",
              "299   -0.000891\n",
              "Length: 300, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAYHOlqXCTs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Capture the average of the word embeddings columns as the word embedding for rare or OOV words \n",
        "rare_embedding = embeds_df.mean(axis=1).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8Rg3gVsHhAi",
        "colab_type": "text"
      },
      "source": [
        "As before, we will create a matrix to hold our 300-dimensional FastText word embeddings. Recall however, that in the Spanish text, we have added Start and Stop tokens to our sentences. We need to convert these tokens to numerical form.\n",
        "\n",
        "We could assign a random vector to both of these. The risk here is that our decoder may predict words that come close to the Stop token, which could cause the translator to stop translating prematurely. \n",
        "\n",
        "To be safe, we will create special columns for the Start and Stop tokens in the embedding matrix. If the translator sees a 1 in the 301st column, it will know that its a Start token. If it sees a 1 in the 302nd column, it will know that it's a Stop token. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rig9SzPehb-",
        "colab_type": "code",
        "outputId": "50fcba5d-ef53-48be-c7aa-17dfa8497f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Create a matrix with a row for every word in our Spanish dictionary, and 302 columns to hold the word embedding for each word,\n",
        "# as well as a column for both the Start and Stop tokens \n",
        "output_embedding_matrix = np.zeros((len(output_words), 302))\n",
        "for i, word in enumerate(output_words):\n",
        "  # if start token, set start token column to 1\n",
        "  if word == \"START_\":\n",
        "    print(i)\n",
        "    output_embedding_matrix[i, 300] = 1\n",
        "\n",
        "  # if stop token, set stop token column to 1\n",
        "  elif word == \"_END\":\n",
        "    print(i)\n",
        "    output_embedding_matrix[i, 301] = 1\n",
        "\n",
        "  # If word not in dictionary, use rare embedding\n",
        "  elif word == \"_UNK_\":\n",
        "    output_embedding_matrix[i, :300] = rare_embedding\n",
        "  else:\n",
        "    # Otherwise, get FastText embedding\n",
        "    try:\n",
        "      embedding_vector = ft_span.get_vector(word)\n",
        "    except KeyError:\n",
        "      embedding_vector = None\n",
        "    if embedding_vector is not None:\n",
        "      # if embedding found, insert into matrix\n",
        "      output_embedding_matrix[i, :300] = embedding_vector\n",
        "    else:\n",
        "      # for OOV words, use rare embedding\n",
        "      output_embedding_matrix[i, :300] = rare_embedding"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BWGIhXYbFkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Memory clean up\n",
        "del ft_span\n",
        "del output_embeddings_index\n",
        "del output_oov_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYho458PIx4j",
        "colab_type": "text"
      },
      "source": [
        "We've successfully converted our English and Spanish vocabularies into matrices using pretrained FastText word embeddings. In the next step, we will complete the data preparation phase by using these matrices to convert our sentences into matrices. Once our sentences are completed transformed into numbers, we can begin assembling our neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYjQU6S8p5Fk",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the Data for our Neural Network\n",
        "\n",
        "\n",
        "In order to train our neural network, we need to convert the text data into numerical data that our neural network can learn from. Up to this point, we have put together the tools necessary to convert our data. Now we will create the matrices that our neural network will use.\n",
        "\n",
        "To begin, we create a set of dictionaries that store a unique index for each word. This will allow our translator to convert words to an index that it can then use to reference the word embeddings matrices created in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7X8v5ur75Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary that stores each word as a unique index\n",
        "input_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(input_words)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzOiTVMIT5Cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary that stores each word as a unique index\n",
        "target_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(output_words)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iT_mHTyrZpX",
        "colab_type": "text"
      },
      "source": [
        "We prepare three matrices. The first matrix is the encoder input matrix. In our case, this matrix will be the numerical representation of each of the English sentences. Each row will represent a sentence. The number of columns is based on the sequence length we determined above, so each column will represent a word. The value at each location will be the index of the word at that position of that sentence. Our neural network will use this index to grab the word's embedding from the input embedding matrix. \n",
        "\n",
        "The second and third matrices will both be for the decoder. The decoder input matrix sentences will start with the START token. The decoder output matrix will be offset by one token, starting with the first token of the sentence. This will train our decoder to predict the next word given the thought vector from the encoder, and the START token. It will then use its output as the input to predict the next token. Training the decoder in this way is called Teacher Forcing. Basically, instead of expecting the decoder to learn to output the whole phrase, we train it at each token and penalize the loss function if the next token is incorrect. \n",
        "\n",
        "For a nice explanation of start and stop tokens, check out this [excellent blog](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) post by Francois Chollet [5]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOlxdESXCYuZ",
        "colab_type": "text"
      },
      "source": [
        "###  Encoder Input Matrix\n",
        "We initialize our encoder input matrix with all zeros. There's a row for every sentence, and a column for each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9WQvNb8yPm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input_data = np.zeros((num_samples, input_seq_len), dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO5rwZ0wC6ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorize the encoder input sentences by storing the word indices in the input matrix\n",
        "# Loop through each sentence\n",
        "for i, input_text in enumerate(eng_sents):\n",
        "  # Loops through each word\n",
        "    for t, word in enumerate(input_text.split()):\n",
        "      if t < input_seq_len:\n",
        "        # if the word is part of our dictionary\n",
        "        if word in input_words:\n",
        "          # store its index in the matrix\n",
        "          encoder_input_data[i, t] = input_token_index[word]\n",
        "        else:\n",
        "          # if the word is not in the input_words matrix, store it as '_UNK_' for unknown\n",
        "          encoder_input_data[i, t] = input_token_index['_UNK_']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VpeHWStDbGq",
        "colab_type": "text"
      },
      "source": [
        "For the decoder, we initialize two matrices. The first is similar to the encoder input data matrix. For the decoder output, since we are trying to predict one word out of many options, we one-hot encode the output words. Instead of representing each word by an instance, we add a column for every word. The column for the word we want gets a 1, the rest get a 0. \n",
        "\n",
        "Also, we offset the output matrix by one token. The input matrix will start with the Start token, while the output matrix will start with the first word of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWCqbgpgyPjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_input_data = np.zeros((num_samples, output_seq_len), dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSl03cHTVgr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_target_data = np.zeros((num_samples, output_seq_len, len(output_words)), dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C2sOewW0TvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorize the decoder input and output sentences\n",
        "# Loop through each sentence\n",
        "for i, output_text in enumerate(span_sents):\n",
        "  # Loop through each word\n",
        "  for t, word in enumerate(output_text.split()):\n",
        "    if word in output_words:\n",
        "      if t < output_seq_len:\n",
        "        decoder_input_data[i, t] = target_token_index[word]\n",
        "\n",
        "      # the decoder output matrix is offset by one token\n",
        "      if t > 0 and t < output_seq_len+1:\n",
        "        decoder_target_data[i, t-1, target_token_index[word]] = 1.\n",
        "    \n",
        "    else:\n",
        "      # if the word is not in the output_words matrix, store it as '_UNK_' for unknown\n",
        "      if t < output_seq_len:\n",
        "        decoder_input_data[i, t] = target_token_index['_UNK_']\n",
        "      if t > 0 and t < output_seq_len+1:\n",
        "        decoder_target_data[i, t-1, target_token_index['_UNK_']] = 1.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No2xwvHyishw",
        "colab_type": "text"
      },
      "source": [
        "## Assembling our Neural Network\n",
        "With our training data vectorized, we are now ready to assemble our encoder-decoder neural network. I've borrowed the basic framework below from Francois Chollet [5], and made tweaks as necessary to apply to our dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0uUhXh0lJH_",
        "colab_type": "text"
      },
      "source": [
        "### Encoder\n",
        "To take advantage of the pretrained word embeddings that we've imported, we create an Embedding layer and set the weights equal to the input_embedding_matrix. We make sure to set the trainable parameter to False so that our word embeddings stay the same throughout the learning process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WJadgubAhJr",
        "colab_type": "code",
        "outputId": "7762b969-f6d1-4119-b83e-1e067f4c385f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "encoder_inputs = Input(shape=(None,))\n",
        "en_x=  Embedding(len(input_words),\n",
        "                            300,\n",
        "                            weights=[input_embedding_matrix],\n",
        "                            trainable=False)(encoder_inputs)\n",
        "encoder = CuDNNLSTM(50, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(en_x)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0ZhwQ7cqOMy",
        "colab_type": "text"
      },
      "source": [
        "### Decoder \n",
        "Our decoder also consists of an embedding layer to hold our pre-trained FastText word embeddings. This time we set the dimension to 302 to hold the extra START and END token columns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMVKb9Oq-zpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dex=  Embedding(len(output_words),\n",
        "                302,\n",
        "                weights=[output_embedding_matrix],\n",
        "                trainable=False)\n",
        "final_dex= dex(decoder_inputs)\n",
        "\n",
        "\n",
        "decoder_lstm = CuDNNLSTM(50, return_sequences=True, return_state=True)\n",
        "\n",
        "decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = Dense(len(output_words), activation='softmax')\n",
        "\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Our neural network model takes both encoder_inputs and decoder_inputs as inputs, with the output being decoder_outputs\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0vd-c-e-zne",
        "colab_type": "code",
        "outputId": "94c07d9d-1f18-4515-8723-5d4076c83c33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 300)    1161900     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 302)    1713246     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)        [(None, 50), (None,  70400       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_3 (CuDNNLSTM)        [(None, None, 50), ( 70800       embedding_3[0][0]                \n",
            "                                                                 cu_dnnlstm_1[0][1]               \n",
            "                                                                 cu_dnnlstm_1[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, None, 5673)   289323      cu_dnnlstm_3[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 3,305,669\n",
            "Trainable params: 430,523\n",
            "Non-trainable params: 2,875,146\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJGf6Wb5FSgA",
        "colab_type": "code",
        "outputId": "d065e711-620f-4901-b5a7-2db6e844f4d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create an EarlyStopping monitor to make sure we don't overfit\n",
        "EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=1, mode='auto', baseline=None, restore_best_weights=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.EarlyStopping at 0x7faa4f196f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImUxRhIJrWhN",
        "colab_type": "text"
      },
      "source": [
        "We fit our model and find that the validation loss seems to stabilize after about 115 epochs. We use a traditional 80/20 split of the train validation data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4L4zdn6jVOi",
        "colab_type": "code",
        "outputId": "7624ab55-5ff3-47e2-faac-4cb260ed384b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x=[encoder_input_data, decoder_input_data], y=decoder_target_data,\n",
        "          batch_size=128,\n",
        "          epochs=130, \n",
        "          validation_split=0.20\n",
        "          )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/130\n",
            "40000/40000 [==============================] - 24s 611us/step - loss: 3.2878 - acc: 0.0963 - val_loss: 2.9953 - val_acc: 0.1275\n",
            "Epoch 2/130\n",
            "40000/40000 [==============================] - 18s 457us/step - loss: 2.8371 - acc: 0.1439 - val_loss: 2.7022 - val_acc: 0.1589\n",
            "Epoch 3/130\n",
            "40000/40000 [==============================] - 18s 456us/step - loss: 2.5803 - acc: 0.1733 - val_loss: 2.5176 - val_acc: 0.1808\n",
            "Epoch 4/130\n",
            "40000/40000 [==============================] - 18s 457us/step - loss: 2.4180 - acc: 0.1900 - val_loss: 2.3844 - val_acc: 0.1933\n",
            "Epoch 5/130\n",
            "40000/40000 [==============================] - 18s 459us/step - loss: 2.3015 - acc: 0.2008 - val_loss: 2.2910 - val_acc: 0.2035\n",
            "Epoch 6/130\n",
            "40000/40000 [==============================] - 18s 458us/step - loss: 2.2101 - acc: 0.2105 - val_loss: 2.2249 - val_acc: 0.2127\n",
            "Epoch 7/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 2.1341 - acc: 0.2195 - val_loss: 2.1571 - val_acc: 0.2222\n",
            "Epoch 8/130\n",
            "40000/40000 [==============================] - 18s 457us/step - loss: 2.0692 - acc: 0.2273 - val_loss: 2.0983 - val_acc: 0.2269\n",
            "Epoch 9/130\n",
            "40000/40000 [==============================] - 18s 457us/step - loss: 2.0106 - acc: 0.2337 - val_loss: 2.0479 - val_acc: 0.2348\n",
            "Epoch 10/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.9603 - acc: 0.2398 - val_loss: 2.0124 - val_acc: 0.2394\n",
            "Epoch 11/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.9152 - acc: 0.2450 - val_loss: 1.9704 - val_acc: 0.2423\n",
            "Epoch 12/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.8747 - acc: 0.2497 - val_loss: 1.9400 - val_acc: 0.2453\n",
            "Epoch 13/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.8405 - acc: 0.2540 - val_loss: 1.9203 - val_acc: 0.2484\n",
            "Epoch 14/130\n",
            "40000/40000 [==============================] - 18s 452us/step - loss: 1.8100 - acc: 0.2573 - val_loss: 1.8878 - val_acc: 0.2527\n",
            "Epoch 15/130\n",
            "40000/40000 [==============================] - 18s 451us/step - loss: 1.7821 - acc: 0.2612 - val_loss: 1.8689 - val_acc: 0.2540\n",
            "Epoch 16/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.7565 - acc: 0.2643 - val_loss: 1.8463 - val_acc: 0.2581\n",
            "Epoch 17/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.7328 - acc: 0.2677 - val_loss: 1.8289 - val_acc: 0.2603\n",
            "Epoch 18/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.7115 - acc: 0.2706 - val_loss: 1.8159 - val_acc: 0.2622\n",
            "Epoch 19/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.6927 - acc: 0.2734 - val_loss: 1.8054 - val_acc: 0.2638\n",
            "Epoch 20/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.6746 - acc: 0.2757 - val_loss: 1.7924 - val_acc: 0.2662\n",
            "Epoch 21/130\n",
            "40000/40000 [==============================] - 18s 451us/step - loss: 1.6556 - acc: 0.2789 - val_loss: 1.7670 - val_acc: 0.2693\n",
            "Epoch 22/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.6350 - acc: 0.2804 - val_loss: 1.7539 - val_acc: 0.2700\n",
            "Epoch 23/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.6190 - acc: 0.2831 - val_loss: 1.7470 - val_acc: 0.2707\n",
            "Epoch 24/130\n",
            "40000/40000 [==============================] - 18s 452us/step - loss: 1.6047 - acc: 0.2856 - val_loss: 1.7295 - val_acc: 0.2743\n",
            "Epoch 25/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.5915 - acc: 0.2873 - val_loss: 1.7211 - val_acc: 0.2751\n",
            "Epoch 26/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.5793 - acc: 0.2896 - val_loss: 1.7190 - val_acc: 0.2758\n",
            "Epoch 27/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.5675 - acc: 0.2918 - val_loss: 1.7087 - val_acc: 0.2784\n",
            "Epoch 28/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.5568 - acc: 0.2934 - val_loss: 1.7072 - val_acc: 0.2785\n",
            "Epoch 29/130\n",
            "40000/40000 [==============================] - 18s 456us/step - loss: 1.5452 - acc: 0.2951 - val_loss: 1.6896 - val_acc: 0.2813\n",
            "Epoch 30/130\n",
            "40000/40000 [==============================] - 18s 462us/step - loss: 1.5349 - acc: 0.2968 - val_loss: 1.6845 - val_acc: 0.2801\n",
            "Epoch 31/130\n",
            "40000/40000 [==============================] - 18s 460us/step - loss: 1.5244 - acc: 0.2982 - val_loss: 1.6782 - val_acc: 0.2830\n",
            "Epoch 32/130\n",
            "40000/40000 [==============================] - 18s 460us/step - loss: 1.5140 - acc: 0.3003 - val_loss: 1.6711 - val_acc: 0.2831\n",
            "Epoch 33/130\n",
            "40000/40000 [==============================] - 18s 459us/step - loss: 1.5025 - acc: 0.3014 - val_loss: 1.6645 - val_acc: 0.2840\n",
            "Epoch 34/130\n",
            "40000/40000 [==============================] - 18s 460us/step - loss: 1.4915 - acc: 0.3030 - val_loss: 1.6535 - val_acc: 0.2858\n",
            "Epoch 35/130\n",
            "40000/40000 [==============================] - 18s 461us/step - loss: 1.4816 - acc: 0.3042 - val_loss: 1.6484 - val_acc: 0.2867\n",
            "Epoch 36/130\n",
            "40000/40000 [==============================] - 18s 459us/step - loss: 1.4728 - acc: 0.3059 - val_loss: 1.6465 - val_acc: 0.2873\n",
            "Epoch 37/130\n",
            "40000/40000 [==============================] - 18s 459us/step - loss: 1.4657 - acc: 0.3073 - val_loss: 1.6429 - val_acc: 0.2875\n",
            "Epoch 38/130\n",
            "40000/40000 [==============================] - 18s 459us/step - loss: 1.4589 - acc: 0.3086 - val_loss: 1.6357 - val_acc: 0.2897\n",
            "Epoch 39/130\n",
            "40000/40000 [==============================] - 18s 461us/step - loss: 1.4529 - acc: 0.3096 - val_loss: 1.6305 - val_acc: 0.2901\n",
            "Epoch 40/130\n",
            "40000/40000 [==============================] - 18s 456us/step - loss: 1.4470 - acc: 0.3105 - val_loss: 1.6235 - val_acc: 0.2919\n",
            "Epoch 41/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.4413 - acc: 0.3119 - val_loss: 1.6257 - val_acc: 0.2916\n",
            "Epoch 42/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.4359 - acc: 0.3130 - val_loss: 1.6176 - val_acc: 0.2930\n",
            "Epoch 43/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.4308 - acc: 0.3137 - val_loss: 1.6179 - val_acc: 0.2938\n",
            "Epoch 44/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.4258 - acc: 0.3150 - val_loss: 1.6175 - val_acc: 0.2932\n",
            "Epoch 45/130\n",
            "40000/40000 [==============================] - 18s 456us/step - loss: 1.4208 - acc: 0.3160 - val_loss: 1.6121 - val_acc: 0.2947\n",
            "Epoch 46/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.4159 - acc: 0.3168 - val_loss: 1.6128 - val_acc: 0.2937\n",
            "Epoch 47/130\n",
            "40000/40000 [==============================] - 18s 452us/step - loss: 1.4114 - acc: 0.3176 - val_loss: 1.6085 - val_acc: 0.2949\n",
            "Epoch 48/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.4062 - acc: 0.3187 - val_loss: 1.6011 - val_acc: 0.2976\n",
            "Epoch 49/130\n",
            "40000/40000 [==============================] - 18s 451us/step - loss: 1.4017 - acc: 0.3195 - val_loss: 1.6033 - val_acc: 0.2955\n",
            "Epoch 50/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.3975 - acc: 0.3208 - val_loss: 1.5996 - val_acc: 0.2971\n",
            "Epoch 51/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.3939 - acc: 0.3211 - val_loss: 1.5987 - val_acc: 0.2973\n",
            "Epoch 52/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.3893 - acc: 0.3221 - val_loss: 1.5965 - val_acc: 0.2983\n",
            "Epoch 53/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.3855 - acc: 0.3227 - val_loss: 1.5939 - val_acc: 0.2983\n",
            "Epoch 54/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.3816 - acc: 0.3236 - val_loss: 1.5894 - val_acc: 0.3006\n",
            "Epoch 55/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.3779 - acc: 0.3243 - val_loss: 1.5919 - val_acc: 0.2997\n",
            "Epoch 56/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.3739 - acc: 0.3247 - val_loss: 1.5850 - val_acc: 0.3005\n",
            "Epoch 57/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.3709 - acc: 0.3256 - val_loss: 1.5859 - val_acc: 0.3010\n",
            "Epoch 58/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.3670 - acc: 0.3260 - val_loss: 1.5932 - val_acc: 0.3011\n",
            "Epoch 59/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.3635 - acc: 0.3266 - val_loss: 1.5814 - val_acc: 0.3016\n",
            "Epoch 60/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.3598 - acc: 0.3275 - val_loss: 1.5817 - val_acc: 0.3017\n",
            "Epoch 61/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.3571 - acc: 0.3282 - val_loss: 1.5764 - val_acc: 0.3021\n",
            "Epoch 62/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.3536 - acc: 0.3283 - val_loss: 1.5767 - val_acc: 0.3028\n",
            "Epoch 63/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.3501 - acc: 0.3294 - val_loss: 1.5811 - val_acc: 0.3018\n",
            "Epoch 64/130\n",
            "40000/40000 [==============================] - 18s 452us/step - loss: 1.3470 - acc: 0.3297 - val_loss: 1.5755 - val_acc: 0.3027\n",
            "Epoch 65/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.3435 - acc: 0.3304 - val_loss: 1.5743 - val_acc: 0.3020\n",
            "Epoch 66/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.3408 - acc: 0.3309 - val_loss: 1.5711 - val_acc: 0.3031\n",
            "Epoch 67/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.3372 - acc: 0.3315 - val_loss: 1.5678 - val_acc: 0.3033\n",
            "Epoch 68/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.3338 - acc: 0.3323 - val_loss: 1.5702 - val_acc: 0.3026\n",
            "Epoch 69/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.3308 - acc: 0.3324 - val_loss: 1.5742 - val_acc: 0.3019\n",
            "Epoch 70/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.3281 - acc: 0.3331 - val_loss: 1.5634 - val_acc: 0.3042\n",
            "Epoch 71/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.3244 - acc: 0.3332 - val_loss: 1.5611 - val_acc: 0.3038\n",
            "Epoch 72/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.3207 - acc: 0.3340 - val_loss: 1.5582 - val_acc: 0.3053\n",
            "Epoch 73/130\n",
            "40000/40000 [==============================] - 18s 455us/step - loss: 1.3172 - acc: 0.3347 - val_loss: 1.5572 - val_acc: 0.3035\n",
            "Epoch 74/130\n",
            "40000/40000 [==============================] - 18s 449us/step - loss: 1.3142 - acc: 0.3352 - val_loss: 1.5562 - val_acc: 0.3046\n",
            "Epoch 75/130\n",
            "40000/40000 [==============================] - 18s 453us/step - loss: 1.3117 - acc: 0.3357 - val_loss: 1.5523 - val_acc: 0.3046\n",
            "Epoch 76/130\n",
            "40000/40000 [==============================] - 19s 478us/step - loss: 1.3085 - acc: 0.3363 - val_loss: 1.5515 - val_acc: 0.3053\n",
            "Epoch 77/130\n",
            "40000/40000 [==============================] - 19s 478us/step - loss: 1.3043 - acc: 0.3367 - val_loss: 1.5482 - val_acc: 0.3053\n",
            "Epoch 78/130\n",
            "40000/40000 [==============================] - 18s 458us/step - loss: 1.3005 - acc: 0.3372 - val_loss: 1.5477 - val_acc: 0.3047\n",
            "Epoch 79/130\n",
            "40000/40000 [==============================] - 18s 457us/step - loss: 1.2975 - acc: 0.3376 - val_loss: 1.5446 - val_acc: 0.3054\n",
            "Epoch 80/130\n",
            "40000/40000 [==============================] - 18s 452us/step - loss: 1.2940 - acc: 0.3382 - val_loss: 1.5446 - val_acc: 0.3062\n",
            "Epoch 81/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.2918 - acc: 0.3391 - val_loss: 1.5486 - val_acc: 0.3055\n",
            "Epoch 82/130\n",
            "40000/40000 [==============================] - 18s 458us/step - loss: 1.2896 - acc: 0.3396 - val_loss: 1.5464 - val_acc: 0.3071\n",
            "Epoch 83/130\n",
            "40000/40000 [==============================] - 18s 458us/step - loss: 1.2884 - acc: 0.3397 - val_loss: 1.5468 - val_acc: 0.3061\n",
            "Epoch 84/130\n",
            "40000/40000 [==============================] - 18s 460us/step - loss: 1.2866 - acc: 0.3405 - val_loss: 1.5466 - val_acc: 0.3064\n",
            "Epoch 85/130\n",
            "40000/40000 [==============================] - 18s 452us/step - loss: 1.2853 - acc: 0.3409 - val_loss: 1.5425 - val_acc: 0.3070\n",
            "Epoch 86/130\n",
            "40000/40000 [==============================] - 18s 454us/step - loss: 1.2843 - acc: 0.3409 - val_loss: 1.5470 - val_acc: 0.3059\n",
            "Epoch 87/130\n",
            "40000/40000 [==============================] - 18s 457us/step - loss: 1.2819 - acc: 0.3415 - val_loss: 1.5452 - val_acc: 0.3065\n",
            "Epoch 88/130\n",
            "40000/40000 [==============================] - 18s 460us/step - loss: 1.2805 - acc: 0.3418 - val_loss: 1.5437 - val_acc: 0.3081\n",
            "Epoch 89/130\n",
            "40000/40000 [==============================] - 19s 464us/step - loss: 1.2790 - acc: 0.3424 - val_loss: 1.5435 - val_acc: 0.3062\n",
            "Epoch 90/130\n",
            "40000/40000 [==============================] - 18s 458us/step - loss: 1.2776 - acc: 0.3427 - val_loss: 1.5411 - val_acc: 0.3074\n",
            "Epoch 91/130\n",
            "40000/40000 [==============================] - 18s 458us/step - loss: 1.2759 - acc: 0.3429 - val_loss: 1.5453 - val_acc: 0.3074\n",
            "Epoch 92/130\n",
            "40000/40000 [==============================] - 19s 463us/step - loss: 1.2743 - acc: 0.3433 - val_loss: 1.5415 - val_acc: 0.3082\n",
            "Epoch 93/130\n",
            "40000/40000 [==============================] - 18s 457us/step - loss: 1.2730 - acc: 0.3438 - val_loss: 1.5454 - val_acc: 0.3064\n",
            "Epoch 94/130\n",
            "40000/40000 [==============================] - 18s 462us/step - loss: 1.2712 - acc: 0.3437 - val_loss: 1.5390 - val_acc: 0.3085\n",
            "Epoch 95/130\n",
            "40000/40000 [==============================] - 19s 468us/step - loss: 1.2696 - acc: 0.3442 - val_loss: 1.5395 - val_acc: 0.3091\n",
            "Epoch 96/130\n",
            "40000/40000 [==============================] - 19s 463us/step - loss: 1.2679 - acc: 0.3447 - val_loss: 1.5418 - val_acc: 0.3079\n",
            "Epoch 97/130\n",
            "40000/40000 [==============================] - 19s 463us/step - loss: 1.2666 - acc: 0.3447 - val_loss: 1.5423 - val_acc: 0.3066\n",
            "Epoch 98/130\n",
            "40000/40000 [==============================] - 18s 461us/step - loss: 1.2647 - acc: 0.3453 - val_loss: 1.5409 - val_acc: 0.3075\n",
            "Epoch 99/130\n",
            "40000/40000 [==============================] - 18s 462us/step - loss: 1.2633 - acc: 0.3454 - val_loss: 1.5413 - val_acc: 0.3082\n",
            "Epoch 100/130\n",
            "40000/40000 [==============================] - 18s 461us/step - loss: 1.2616 - acc: 0.3458 - val_loss: 1.5403 - val_acc: 0.3075\n",
            "Epoch 101/130\n",
            "40000/40000 [==============================] - 19s 463us/step - loss: 1.2600 - acc: 0.3457 - val_loss: 1.5359 - val_acc: 0.3092\n",
            "Epoch 102/130\n",
            "40000/40000 [==============================] - 18s 462us/step - loss: 1.2581 - acc: 0.3461 - val_loss: 1.5387 - val_acc: 0.3077\n",
            "Epoch 103/130\n",
            "40000/40000 [==============================] - 19s 464us/step - loss: 1.2569 - acc: 0.3464 - val_loss: 1.5364 - val_acc: 0.3088\n",
            "Epoch 104/130\n",
            "40000/40000 [==============================] - 18s 461us/step - loss: 1.2549 - acc: 0.3467 - val_loss: 1.5381 - val_acc: 0.3085\n",
            "Epoch 105/130\n",
            "40000/40000 [==============================] - 18s 458us/step - loss: 1.2532 - acc: 0.3472 - val_loss: 1.5347 - val_acc: 0.3093\n",
            "Epoch 106/130\n",
            "40000/40000 [==============================] - 18s 461us/step - loss: 1.2520 - acc: 0.3474 - val_loss: 1.5372 - val_acc: 0.3089\n",
            "Epoch 107/130\n",
            "40000/40000 [==============================] - 19s 464us/step - loss: 1.2510 - acc: 0.3477 - val_loss: 1.5353 - val_acc: 0.3086\n",
            "Epoch 108/130\n",
            "40000/40000 [==============================] - 18s 461us/step - loss: 1.2496 - acc: 0.3479 - val_loss: 1.5362 - val_acc: 0.3091\n",
            "Epoch 109/130\n",
            "40000/40000 [==============================] - 19s 466us/step - loss: 1.2478 - acc: 0.3484 - val_loss: 1.5371 - val_acc: 0.3081\n",
            "Epoch 110/130\n",
            "40000/40000 [==============================] - 19s 471us/step - loss: 1.2465 - acc: 0.3488 - val_loss: 1.5351 - val_acc: 0.3092\n",
            "Epoch 111/130\n",
            "40000/40000 [==============================] - 18s 461us/step - loss: 1.2450 - acc: 0.3486 - val_loss: 1.5321 - val_acc: 0.3093\n",
            "Epoch 112/130\n",
            "40000/40000 [==============================] - 18s 462us/step - loss: 1.2436 - acc: 0.3489 - val_loss: 1.5325 - val_acc: 0.3093\n",
            "Epoch 113/130\n",
            "40000/40000 [==============================] - 19s 466us/step - loss: 1.2420 - acc: 0.3491 - val_loss: 1.5334 - val_acc: 0.3082\n",
            "Epoch 114/130\n",
            "40000/40000 [==============================] - 19s 465us/step - loss: 1.2406 - acc: 0.3497 - val_loss: 1.5371 - val_acc: 0.3080\n",
            "Epoch 115/130\n",
            "40000/40000 [==============================] - 19s 463us/step - loss: 1.2389 - acc: 0.3499 - val_loss: 1.5312 - val_acc: 0.3088\n",
            "Epoch 116/130\n",
            "40000/40000 [==============================] - 19s 465us/step - loss: 1.2376 - acc: 0.3502 - val_loss: 1.5306 - val_acc: 0.3092\n",
            "Epoch 117/130\n",
            "40000/40000 [==============================] - 19s 468us/step - loss: 1.2364 - acc: 0.3500 - val_loss: 1.5321 - val_acc: 0.3093\n",
            "Epoch 118/130\n",
            "40000/40000 [==============================] - 19s 464us/step - loss: 1.2350 - acc: 0.3506 - val_loss: 1.5316 - val_acc: 0.3092\n",
            "Epoch 119/130\n",
            "40000/40000 [==============================] - 19s 464us/step - loss: 1.2337 - acc: 0.3505 - val_loss: 1.5291 - val_acc: 0.3098\n",
            "Epoch 120/130\n",
            "40000/40000 [==============================] - 19s 465us/step - loss: 1.2322 - acc: 0.3510 - val_loss: 1.5328 - val_acc: 0.3087\n",
            "Epoch 121/130\n",
            "40000/40000 [==============================] - 19s 467us/step - loss: 1.2309 - acc: 0.3509 - val_loss: 1.5285 - val_acc: 0.3096\n",
            "Epoch 122/130\n",
            "40000/40000 [==============================] - 19s 466us/step - loss: 1.2294 - acc: 0.3517 - val_loss: 1.5282 - val_acc: 0.3096\n",
            "Epoch 123/130\n",
            "40000/40000 [==============================] - 19s 465us/step - loss: 1.2277 - acc: 0.3520 - val_loss: 1.5298 - val_acc: 0.3094\n",
            "Epoch 124/130\n",
            "40000/40000 [==============================] - 19s 465us/step - loss: 1.2269 - acc: 0.3523 - val_loss: 1.5299 - val_acc: 0.3104\n",
            "Epoch 125/130\n",
            "40000/40000 [==============================] - 18s 460us/step - loss: 1.2253 - acc: 0.3527 - val_loss: 1.5284 - val_acc: 0.3095\n",
            "Epoch 126/130\n",
            "40000/40000 [==============================] - 18s 458us/step - loss: 1.2244 - acc: 0.3526 - val_loss: 1.5281 - val_acc: 0.3102\n",
            "Epoch 127/130\n",
            "40000/40000 [==============================] - 18s 457us/step - loss: 1.2231 - acc: 0.3526 - val_loss: 1.5269 - val_acc: 0.3098\n",
            "Epoch 128/130\n",
            "40000/40000 [==============================] - 18s 456us/step - loss: 1.2212 - acc: 0.3529 - val_loss: 1.5262 - val_acc: 0.3090\n",
            "Epoch 129/130\n",
            "40000/40000 [==============================] - 18s 456us/step - loss: 1.2203 - acc: 0.3533 - val_loss: 1.5241 - val_acc: 0.3102\n",
            "Epoch 130/130\n",
            "40000/40000 [==============================] - 18s 460us/step - loss: 1.2192 - acc: 0.3537 - val_loss: 1.5259 - val_acc: 0.3107\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faa4f196eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG1a0Q4VAhGY",
        "colab_type": "code",
        "outputId": "e71e9732-9af7-4b80-aac3-06146382d49e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "encoder_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, None, 300)         1161900   \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     [(None, 50), (None, 50),  70400     \n",
            "=================================================================\n",
            "Total params: 1,232,300\n",
            "Trainable params: 70,400\n",
            "Non-trainable params: 1,161,900\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-2K0NjpNeC9",
        "colab_type": "text"
      },
      "source": [
        "## Inference Mode\n",
        "Now that we have trained an encoder-decoder model, we want to use this model to translate, or infer, new sentences. We will use the internal states that were generated during training, to take a new sentence and have the encoder-decoder     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slOVaimI_0To",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_state_input_h = Input(shape=(50,))\n",
        "decoder_state_input_c = Input(shape=(50,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "final_dex2= dex(decoder_inputs)\n",
        "\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8DNBgUTGvs",
        "colab_type": "text"
      },
      "source": [
        "Inference loop borrowed from Francois Chollet and tweaked for our needs [5]. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoBqLjVN_0SL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        if sampled_token_index == 51:\n",
        "          print(reverse_target_char_index[sampled_token_index])\n",
        "\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(decoded_sentence.split()) > output_seq_len):\n",
        "          #print(sampled_char)\n",
        "          #print(len(decoded_sentence.split()))\n",
        "          stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah152a7cTOrE",
        "colab_type": "text"
      },
      "source": [
        "Let's take a simple test phrase, and convert it to a valid input matrix, and then use our encoder-decoder to translate it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A0LP39kTdCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_phrase = \"i see you\"\n",
        "\n",
        "# Create a dummy input vector to hold out test_phrase\n",
        "test_phrase_input_vector = np.zeros((1, input_seq_len), dtype='float32')\n",
        "test_phrase__output_vector = np.zeros((1, output_seq_len, len(output_words)), dtype='float32')\n",
        "\n",
        "# Convert each word in test phrase into the appropriate vector\n",
        "for t, word in enumerate(test_phrase.split()):\n",
        "  try:\n",
        "    tok_ind = input_token_index[word]\n",
        "  except KeyError:\n",
        "    tok_ind = None\n",
        "  \n",
        "  if tok_ind is not None:\n",
        "    test_phrase_input_vector[0, t] = input_token_index[word]\n",
        "  else:\n",
        "    test_phrase_input_vector[0, t] = input_token_index['_UNK_']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz2oIxYZdPpG",
        "colab_type": "code",
        "outputId": "facfbb8e-d48c-4f4b-c072-3e828c722993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(test_phrase)\n",
        "decoded_sent = decode_sequence(test_phrase_input_vector)\n",
        "\n",
        "print(decoded_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i see you\n",
            " te veo _END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVYHIe8rfO6d",
        "colab_type": "text"
      },
      "source": [
        "For the simple phrase \"I see you,\" it looks like our translator guessed spot on with \"Te veo.\" Let's randomly sample our training sentences and see how the translator fares. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YReda5R9Tc_B",
        "colab_type": "code",
        "outputId": "5e74fe87-f878-4d15-ba2c-6fc13da26a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "rand_idx = random.sample(range(0, num_samples-1), 10)\n",
        "\n",
        "for seq_index in rand_idx:\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sents[seq_index: seq_index + 1])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: ['he has a cold']\n",
            "Decoded sentence:  él tiene un frío _END\n",
            "-\n",
            "Input sentence: ['tom went blind']\n",
            "Decoded sentence:  tom fue a _UNK_ _END\n",
            "-\n",
            "Input sentence: ['ive got a touch of the flu']\n",
            "Decoded sentence:  tengo una _UNK_ en la casa _END\n",
            "-\n",
            "Input sentence: ['part of his story is true']\n",
            "Decoded sentence:  por qué es su historia es verdad _END\n",
            "-\n",
            "Input sentence: ['mary is my niece']\n",
            "Decoded sentence:  mary es mi madre _END\n",
            "-\n",
            "Input sentence: ['choose one person']\n",
            "Decoded sentence:  _UNK_ una persona _END\n",
            "-\n",
            "Input sentence: ['tom ran away']\n",
            "Decoded sentence:  tom se quedó _END\n",
            "-\n",
            "Input sentence: ['i want to make her happy']\n",
            "Decoded sentence:  quiero _UNK_ con su ayuda _END\n",
            "-\n",
            "Input sentence: ['he isnt afraid to die']\n",
            "Decoded sentence:  él no tiene miedo de morir _END\n",
            "-\n",
            "Input sentence: ['did you buy a car']\n",
            "Decoded sentence:  compraste un coche _END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4cL-iJAfmRg",
        "colab_type": "text"
      },
      "source": [
        "If you know any Spanish, you'll see that sometimes the translator is almost spot on, and other times the translator veers a little off course. It's nowhere near Google translate production quality just yet, but overall, it's a good start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkQSQv1hOfl_",
        "colab_type": "text"
      },
      "source": [
        "# Sources \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osk7IWXFOj28",
        "colab_type": "text"
      },
      "source": [
        "[1] https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/feature%20engineering%20text%20data/Feature%20Engineering%20Text%20Data%20-%20Traditional%20Strategies.ipynb\n",
        "\n",
        "[2] https://en.wikipedia.org/wiki/Morphological_typology\n",
        "\n",
        "[3] https://fasttext.cc/docs/en/pretrained-vectors.html\n",
        "\n",
        "[4] https://www.kaggle.com/sbongo/do-pretrained-embeddings-give-you-the-extra-edge\n",
        "\n",
        "[5] https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRpvkei_0K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39MbzVHO_0IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWDRhldmAhEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsxCaEakAhA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5WalW1cw2T1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer_vec = ft.get_vector('dogs') - ft.get_vector('dog')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxVtJCvezL_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer_vec2 = ft.get_vector('cat') - ft.get_vector('cats')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEVqrrV4zT47",
        "colab_type": "code",
        "outputId": "1db21ec5-c13a-4ce7-a648-42253de3617d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ft.cosine_similarities(answer_vec, [answer_vec2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.7573222], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWqA4ToSxsOX",
        "colab_type": "code",
        "outputId": "a050cf89-fdea-47c1-e5a4-6e0dca44ee7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "ft.most_similar(positive=[answer_vec], topn=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('OCCs', 0.2844681143760681), ('SPMs', 0.2560500204563141)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRMdmTKKzPle",
        "colab_type": "code",
        "outputId": "5f56f981-d394-41eb-df82-3dc74111e550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "ft.most_similar(positive=[answer_vec2], topn=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Units', 0.2607120871543884), ('busloads', 0.24602153897285461)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    }
  ]
}